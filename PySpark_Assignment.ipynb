{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Assignment\").getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "type of words : <class 'pyspark.rdd.RDD'>\nNumber of words : 28\n['Your', 'time', 'is', 'limited,', 'so', \"don't\", 'waste', 'it', 'living', 'someone', \"else's\", 'life.', \"Don't\", 'be', 'trapped', 'by', 'dogma', '–', 'which', 'is', 'living', 'with', 'the', 'results', 'of', 'other', \"people's\", 'thinking.']\n"
     ]
    }
   ],
   "source": [
    "#1.Creating Resilient Distributed Dataset in 3 different ways.\n",
    "\n",
    "#Method 1:Parallelize\n",
    "\n",
    "text = \"Your time is limited, so don't waste it living someone else's life. Don't be trapped by dogma – which is living with the results of other people's thinking.\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(text,2)\n",
    "typ = type(words)\n",
    "word = words.collect()\n",
    "count = words.count()\n",
    "\n",
    "print(\"type of words :\",typ)\n",
    "print(\"Number of words :\",count)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "type of Data <class 'list'>\n['The greatest glory in living lies not in never falling, but in rising every time we fall.', 'The way to get started is to quit talking and begin doing.', \"Your time is limited, so don't waste it living someone else's life. Don't be trapped by dogma – which is living with the results of other people's thinking.\", 'If life were predictable it would cease to be life, and be without flavor.', \"If you look at what you have in life, you'll always have more. If you look at what you don't have in life, you'll never have enough.\", \"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.    \", \"Life is what happens when you're busy making other plans.\"]\n"
     ]
    }
   ],
   "source": [
    "#Method 2: Using Data source\n",
    "\n",
    "data = spark.sparkContext.textFile(\"textfile.txt\")\n",
    "Data = data.collect()\n",
    "t2 = type(Data)\n",
    "print(\"type of Data\",t2)\n",
    "print(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n['so', 'someone']\n"
     ]
    }
   ],
   "source": [
    "#Method 3: Using Pipelined RDD\n",
    "info = words.filter(lambda word:word.startswith('s'))\n",
    "inf = info.collect()\n",
    "print(type(info))\n",
    "print(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "#2.Read a text file and count the number of words in the file using RDD operations.\n",
    "\n",
    "Total_words=data.flatMap(lambda s:s.split(\" \"))\n",
    "Total_words.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 2),\n",
       " ('greatest', 1),\n",
       " ('glory', 1),\n",
       " ('in', 5),\n",
       " ('living', 3),\n",
       " ('never', 2),\n",
       " ('falling,', 1),\n",
       " ('but', 1),\n",
       " ('rising', 1),\n",
       " ('we', 1),\n",
       " ('fall.', 1),\n",
       " ('way', 1),\n",
       " ('started', 1),\n",
       " ('is', 4),\n",
       " ('quit', 1),\n",
       " ('talking', 1),\n",
       " ('doing.', 1),\n",
       " ('life.', 1),\n",
       " (\"Don't\", 1),\n",
       " ('dogma', 1),\n",
       " ('results', 1),\n",
       " ('of', 1),\n",
       " ('other', 2),\n",
       " (\"people's\", 1),\n",
       " ('thinking.', 1),\n",
       " ('would', 1),\n",
       " ('cease', 1),\n",
       " ('life,', 3),\n",
       " ('look', 2),\n",
       " ('at', 2),\n",
       " ('have', 4),\n",
       " (\"you'll\", 2),\n",
       " ('always', 1),\n",
       " ('set', 1),\n",
       " ('ridiculously', 1),\n",
       " ('high', 1),\n",
       " ('above', 1),\n",
       " ('success.', 1),\n",
       " ('Life', 1),\n",
       " ('when', 1),\n",
       " (\"you're\", 1),\n",
       " ('making', 1),\n",
       " ('lies', 1),\n",
       " ('not', 1),\n",
       " ('every', 1),\n",
       " ('time', 2),\n",
       " ('to', 3),\n",
       " ('get', 1),\n",
       " ('and', 3),\n",
       " ('begin', 1),\n",
       " ('Your', 1),\n",
       " ('limited,', 1),\n",
       " ('so', 1),\n",
       " (\"don't\", 2),\n",
       " ('waste', 1),\n",
       " ('it', 2),\n",
       " ('someone', 1),\n",
       " (\"else's\", 2),\n",
       " ('be', 3),\n",
       " ('trapped', 1),\n",
       " ('by', 1),\n",
       " ('–', 1),\n",
       " ('which', 1),\n",
       " ('with', 1),\n",
       " ('the', 1),\n",
       " ('If', 4),\n",
       " ('life', 1),\n",
       " ('were', 1),\n",
       " ('predictable', 1),\n",
       " ('without', 1),\n",
       " ('flavor.', 1),\n",
       " ('you', 6),\n",
       " ('what', 3),\n",
       " ('more.', 1),\n",
       " ('enough.', 1),\n",
       " ('your', 1),\n",
       " ('goals', 1),\n",
       " (\"it's\", 1),\n",
       " ('a', 1),\n",
       " ('failure,', 1),\n",
       " ('will', 1),\n",
       " ('fail', 1),\n",
       " ('everyone', 1),\n",
       " ('happens', 1),\n",
       " ('busy', 1),\n",
       " ('plans.', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "#3.Write a program to find the word frequency in a given file.\n",
    "datainfo=spark.sparkContext.textFile(\"textfile.txt\")\n",
    "datainfo=datainfo.flatMap(lambda x:x.split())\n",
    "content=datainfo.map(lambda c:(c,1))\n",
    "content.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['THE GREATEST GLORY IN LIVING LIES NOT IN NEVER FALLING, BUT IN RISING EVERY TIME WE FALL.',\n",
       " 'THE WAY TO GET STARTED IS TO QUIT TALKING AND BEGIN DOING.',\n",
       " \"YOUR TIME IS LIMITED, SO DON'T WASTE IT LIVING SOMEONE ELSE'S LIFE. DON'T BE TRAPPED BY DOGMA – WHICH IS LIVING WITH THE RESULTS OF OTHER PEOPLE'S THINKING.\",\n",
       " 'IF LIFE WERE PREDICTABLE IT WOULD CEASE TO BE LIFE, AND BE WITHOUT FLAVOR.',\n",
       " \"IF YOU LOOK AT WHAT YOU HAVE IN LIFE, YOU'LL ALWAYS HAVE MORE. IF YOU LOOK AT WHAT YOU DON'T HAVE IN LIFE, YOU'LL NEVER HAVE ENOUGH.\",\n",
       " \"IF YOU SET YOUR GOALS RIDICULOUSLY HIGH AND IT'S A FAILURE, YOU WILL FAIL ABOVE EVERYONE ELSE'S SUCCESS.    \",\n",
       " \"LIFE IS WHAT HAPPENS WHEN YOU'RE BUSY MAKING OTHER PLANS.\"]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "#4. Write a program to convert all words in a file to uppercase.\n",
    "\n",
    "data.map(lambda c:c.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['the greatest glory in living lies not in never falling, but in rising every time we fall.',\n",
       " 'the way to get started is to quit talking and begin doing.',\n",
       " \"your time is limited, so don't waste it living someone else's life. don't be trapped by dogma – which is living with the results of other people's thinking.\",\n",
       " 'if life were predictable it would cease to be life, and be without flavor.',\n",
       " \"if you look at what you have in life, you'll always have more. if you look at what you don't have in life, you'll never have enough.\",\n",
       " \"if you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.    \",\n",
       " \"life is what happens when you're busy making other plans.\"]"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "#5. Write a program to convert all words in a file to lowercase.\n",
    "\n",
    "data.map(lambda c:c.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"The Greatest Glory In Living Lies Not In Never Falling, But In Rising Every Time We Fall. The Way To Get Started Is To Quit Talking And Begin Doing. Your Time Is Limited, So Don't Waste It Living Someone Else's Life. Don't Be Trapped By Dogma – Which Is Living With The Results Of Other People's Thinking. If Life Were Predictable It Would Cease To Be Life, And Be Without Flavor. If You Look At What You Have In Life, You'll Always Have More. If You Look At What You Don't Have In Life, You'll Never Have Enough. If You Set Your Goals Ridiculously High And It's A Failure, You Will Fail Above Everyone Else's Success.     Life Is What Happens When You're Busy Making Other Plans.\""
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "#6. Write a program to capitalize first letter of each words in file (use string capitalize() method)\n",
    "\n",
    "Uppercase = data.flatMap(lambda a:a.split(\" \")).map(lambda c:c.capitalize()).collect()\n",
    "\" \".join(Uppercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ridiculously'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "#7.Find the longest length of word from given set of words.\n",
    "\n",
    "longestlength=data.flatMap(lambda x:x.split(\" \"))\n",
    "longestlength.map(lambda nu:(len(nu),nu)).max()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('BDA', 6001), ('VLSI', 2001), ('VLSI', 2002), ('CC', 5001), ('BDA', 6002), ('HDA', 9001), ('ES', 3001), ('ES', 3002), ('MSc', 4001), ('MSc', 4002), ('CC', 5002), ('CC', 5003), ('MS', 1001), ('MS', 1002)]\n"
     ]
    }
   ],
   "source": [
    "#8. Map the Registration numbers to corresponding branch.\n",
    "#6000 series BDA, 9000 series HDA, 1000 series ML, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC.\n",
    "#Given registration number, generate a key-value pair of Registration Number and Corresponding Branch.\n",
    "\n",
    "USN = [6001,2001,2002,5001,6002,9001,3001,3002,4001,4002,5002,5003,1001,1002]\n",
    "context=spark.sparkContext.parallelize(USN,2)\n",
    "classify=context.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \n",
    "        else ('MS',reg) if reg>1000 and reg<2000\n",
    "        else ('ES',reg) if reg>3000 and reg<4000\n",
    "        else ('MSc',reg) if reg>4000 and reg<5000\n",
    "        else ('CC',reg) if reg>5000 and reg<6000\n",
    "        else ('BDA',reg) if reg>6000 and reg<7000\n",
    "        else ('HDA',reg))\n",
    "classified_usn = classify.collect()\n",
    "print(classified_usn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maximum number 90\nMinimum number 12\nSum 1364\nMean 48.714285714285715\n"
     ]
    }
   ],
   "source": [
    "# 9.\n",
    "#Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers.\n",
    "#One line may contain one or more numbers. Find the maximum, minimum, sum and mean of numbers.\n",
    "num=spark.sparkContext.textFile(\"Numbers.txt\")\n",
    "num_rdd=num.flatMap(lambda z:z.split(\" \")).map(lambda c:int(c))\n",
    "print(\"Maximum number\",num_rdd.max())\n",
    "print(\"Minimum number\",num_rdd.min())\n",
    "print(\"Sum\",num_rdd.sum())\n",
    "print(\"Mean\",num_rdd.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Thejas', '25-5-1997', '201046013', 'thejas@gmail.com', 'AP'],\n",
       " ['Rakhesh', '22-03-1997', '201051023', 'rakhi@gmail.com', 'MH'],\n",
       " ['Shanthkumar', '14-03-1999', '201046049', 'shanthu@gmail.com', 'GJ'],\n",
       " ['kiran', '21-10-1997', '2010156001', 'kiran@gmail.com', 'WestBengal'],\n",
       " ['Anupam', '05-09-1993', '201056002', 'anupam@gmail.com', 'RJ'],\n",
       " ['Subrahmanyan', '04-12-1997', '201046007', 'sub@gmail.com', 'KA'],\n",
       " ['Abhishek', '15-10-1997', '201051007', 'abhi@gmail.com', 'DL'],\n",
       " ['Chandan', '13-02-1997', '201046003', 'chandan@gmail.com', 'PB']]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "#10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone,\n",
    "# email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc.\n",
    "# Compress the citizen.txt file by changing full state name to state code.\n",
    "\n",
    "Citizen_details = spark.sparkContext.textFile(\"citizen.txt\")\n",
    "State_code = spark.sparkContext.textFile(\"State_codes.txt\")\n",
    "\n",
    "details_rdd = Citizen_details.map(lambda x:x.split(\",\")).collect()\n",
    "code_rdd= State_code.map(lambda y:y.split(\",\")).collect()\n",
    "\n",
    "for i in range(len(details_rdd)):\n",
    "    for j in range(len(code_rdd)):  \n",
    "        if details_rdd[i][4]==code_rdd[j][0]:\n",
    "            details_rdd[i][4]=code_rdd[j][1]\n",
    "details_rdd     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Karnataka': 'KA',\n",
       " 'Delhi': 'DL',\n",
       " 'Gujarat': 'GJ',\n",
       " 'Maharashtra': 'MH',\n",
       " 'Rajasthan': 'RJ',\n",
       " 'Punjab': 'PB',\n",
       " 'AndhraPradesh': 'AP',\n",
       " 'westBengal': 'WB'}"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "state = spark.sparkContext.textFile('State_codes.txt')\n",
    "stateKey = state.map(lambda word: (word.split(',')[0], word.split(',')[1]))\n",
    "\n",
    "Statecode_dict = {}\n",
    "for val in stateKey.collect():\n",
    "    Statecode_dict[val[0]] = val[1]\n",
    "    \n",
    "Statecode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Thejas,25-5-1997,201046013,thejas@gmail.com,AndhraPradesh', 'Rakhesh,22-03-1997,201051023,rakhi@gmail.com,Maharashtra', 'Shanthkumar,14-03-1999,201046049,shanthu@gmail.com,Gujarat', 'kiran,21-10-1997,2010156001,kiran@gmail.com,WestBengal', 'Anupam,05-09-1993,201056002,anupam@gmail.com,Rajasthan', 'Subrahmanyan,04-12-1997,201046007,sub@gmail.com,Karnataka', 'Abhishek,15-10-1997,201051007,abhi@gmail.com,Delhi', 'Chandan,13-02-1997,201046003,chandan@gmail.com,Punjab']\n"
     ]
    }
   ],
   "source": [
    "mapData = spark.sparkContext.broadcast(Statecode_dict)\n",
    "\n",
    "city = spark.sparkContext.textFile('citizen.txt')\n",
    "print(city.collect())\n",
    "def compress(state,codes):\n",
    "    splitData = state.split(',')  \n",
    "    print(splitData)\n",
    "    splitData[4] = codes.value.get(splitData[4])\n",
    "    newData = ' '\n",
    "    newData = newData.join(splitData)\n",
    "    \n",
    "    return newData\n",
    "    \n",
    "Citizen = city.map(lambda data: compress(data,mapData))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}