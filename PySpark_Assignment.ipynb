{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# To make pyspark importable as a regular library.\n",
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Assignment\").getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of words : <class 'pyspark.rdd.RDD'>\n",
      "Number of words : 27\n",
      "['If', 'you', 'look', 'at', 'what', 'you', 'have', 'in', 'life,', \"you'll\", 'always', 'have', 'more.', 'If', 'you', 'look', 'at', 'what', 'you', \"don't\", 'have', 'in', 'life,', \"you'll\", 'never', 'have', 'enough.']\n"
     ]
    }
   ],
   "source": [
    "#1.Creating Resilient Distributed Dataset in 3 different ways.\n",
    "\n",
    "#Method 1:Parallelize\n",
    "\n",
    "text = \"If you look at what you have in life, you'll always have more. If you look at what you don't have in life, you'll never have enough.\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(text,2)\n",
    "typ = type(words)\n",
    "word = words.collect()\n",
    "count = words.count()\n",
    "\n",
    "print(\"type of words :\",typ)\n",
    "print(\"Number of words :\",count)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of Data <class 'list'>\n",
      "[\"Nature's first green is gold,\", 'Her hardest hue to hold.', \"Her early leaf's a flower;\", 'But only so an hour.', 'Then leaf subsides to leaf.', 'So Eden sank to grief,', 'So dawn goes down to day.', 'Nothing gold can stay.']\n"
     ]
    }
   ],
   "source": [
    "#Method 2: Using Data source\n",
    "\n",
    "data = spark.sparkContext.textFile(\"textfile.txt\")\n",
    "Data = data.collect()\n",
    "t2 = type(Data)\n",
    "print(\"type of Data\",t2)\n",
    "print(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'life,', 'look', 'life,']\n"
     ]
    }
   ],
   "source": [
    "#Method 3: Using Pipelined RDD\n",
    "info = words.filter(lambda word:word.startswith('l'))\n",
    "inf = info.collect()\n",
    "type(info)\n",
    "print(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.Read a text file and count the number of words in the file using RDD operations.\n",
    "\n",
    "Total_words=data.flatMap(lambda s:s.split(\" \"))\n",
    "Total_words.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('green', 1),\n",
       " ('is', 1),\n",
       " ('gold,', 1),\n",
       " ('Her', 2),\n",
       " ('hardest', 1),\n",
       " ('hue', 1),\n",
       " (\"leaf's\", 1),\n",
       " ('flower;', 1),\n",
       " ('But', 1),\n",
       " ('only', 1),\n",
       " ('an', 1),\n",
       " ('hour.', 1),\n",
       " ('Then', 1),\n",
       " ('leaf', 1),\n",
       " ('subsides', 1),\n",
       " ('leaf.', 1),\n",
       " ('Eden', 1),\n",
       " ('grief,', 1),\n",
       " ('dawn', 1),\n",
       " ('goes', 1),\n",
       " ('down', 1),\n",
       " ('Nothing', 1),\n",
       " ('gold', 1),\n",
       " (\"Nature's\", 1),\n",
       " ('first', 1),\n",
       " ('to', 4),\n",
       " ('hold.', 1),\n",
       " ('early', 1),\n",
       " ('a', 1),\n",
       " ('so', 1),\n",
       " ('So', 2),\n",
       " ('sank', 1),\n",
       " ('day.', 1),\n",
       " ('can', 1),\n",
       " ('stay.', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.Write a program to find the word frequency in a given file.\n",
    "datainfo=spark.sparkContext.textFile(\"textfile.txt\")\n",
    "datainfo=datainfo.flatMap(lambda x:x.split())\n",
    "content=datainfo.map(lambda c:(c,1))\n",
    "content.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NATURE'S FIRST GREEN IS GOLD,\",\n",
       " 'HER HARDEST HUE TO HOLD.',\n",
       " \"HER EARLY LEAF'S A FLOWER;\",\n",
       " 'BUT ONLY SO AN HOUR.',\n",
       " 'THEN LEAF SUBSIDES TO LEAF.',\n",
       " 'SO EDEN SANK TO GRIEF,',\n",
       " 'SO DAWN GOES DOWN TO DAY.',\n",
       " 'NOTHING GOLD CAN STAY.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Write a program to convert all words in a file to uppercase.\n",
    "\n",
    "data.map(lambda c:c.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"nature's first green is gold,\",\n",
       " 'her hardest hue to hold.',\n",
       " \"her early leaf's a flower;\",\n",
       " 'but only so an hour.',\n",
       " 'then leaf subsides to leaf.',\n",
       " 'so eden sank to grief,',\n",
       " 'so dawn goes down to day.',\n",
       " 'nothing gold can stay.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Write a program to convert all words in a file to lowercase.\n",
    "\n",
    "data.map(lambda c:c.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nature's First Green Is Gold, Her Hardest Hue To Hold. Her Early Leaf's A Flower; But Only So An Hour. Then Leaf Subsides To Leaf. So Eden Sank To Grief, So Dawn Goes Down To Day. Nothing Gold Can Stay.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. Write a program to capitalize first letter of each words in file (use string capitalize() method)\n",
    "\n",
    "Uppercase = data.flatMap(lambda a:a.split(\" \")).map(lambda c:c.capitalize()).collect()\n",
    "\" \".join(Uppercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subsides'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.Find the longest length of word from given set of words.\n",
    "\n",
    "longestlength=data.flatMap(lambda x:x.split(\" \"))\n",
    "longestlength.map(lambda nu:(len(nu),nu)).max()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BDA', 6001), ('VLSI', 2001), ('VLSI', 2002), ('CC', 5001), ('BDA', 6002), ('HDA', 9001), ('ES', 3001), ('ES', 3002), ('MSc', 4001), ('MSc', 4002), ('CC', 5002), ('CC', 5003), ('MS', 1001), ('MS', 1002)]\n"
     ]
    }
   ],
   "source": [
    "#8. Map the Registration numbers to corresponding branch.\n",
    "#6000 series BDA, 9000 series HDA, 1000 series ML, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC.\n",
    "#Given registration number, generate a key-value pair of Registration Number and Corresponding Branch.\n",
    "\n",
    "USN = [6001,2001,2002,5001,6002,9001,3001,3002,4001,4002,5002,5003,1001,1002]\n",
    "context=spark.sparkContext.parallelize(USN,2)\n",
    "classify=context.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \n",
    "        else ('MS',reg) if reg>1000 and reg<2000\n",
    "        else ('ES',reg) if reg>3000 and reg<4000\n",
    "        else ('MSc',reg) if reg>4000 and reg<5000\n",
    "        else ('CC',reg) if reg>5000 and reg<6000\n",
    "        else ('BDA',reg) if reg>6000 and reg<7000\n",
    "        else ('HDA',reg))\n",
    "classified_usn = classify.collect()\n",
    "print(classified_usn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Subrahmanyan', '04-12-1997', '201046007', 'sub@gmail.com', 'KA'],\n",
       " ['Abhishek', '15-10-1997', '201051007', 'abhi@gmail.com', 'DL'],\n",
       " ['Thejas', '25-5-1997', '201046013', 'thejas@gmail.com', 'AP'],\n",
       " ['Rakhesh', '22-03-1997', '201051023', 'rakhi@gmail.com', 'MH'],\n",
       " ['Shanthkumar', '14-03-1999', '201046049', 'shanthu@gmail.com', 'GJ'],\n",
       " ['kiran', '21-10-1997', '2010156001', 'kiran@gmail.com', 'WestBengal'],\n",
       " ['Anupam', '05-09-1993', '201056002', 'anupam@gmail.com', 'RJ'],\n",
       " ['Chandan', '13-02-1997', '201046003', 'chandan@gmail.com', 'PB']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone,\n",
    "# email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc.\n",
    "# Compress the citizen.txt file by changing full state name to state code.\n",
    "\n",
    "Citizen_details = spark.sparkContext.textFile(\"citizen.txt\")\n",
    "State_code = spark.sparkContext.textFile(\"State_codes.txt\")\n",
    "\n",
    "details_rdd = Citizen_details.map(lambda x:x.split(\",\")).collect()\n",
    "code_rdd= State_code.map(lambda y:y.split(\",\")).collect()\n",
    "\n",
    "for i in range(len(details_rdd)):\n",
    "    for j in range(len(code_rdd)):  \n",
    "        if details_rdd[i][4]==code_rdd[j][0]:\n",
    "            details_rdd[i][4]=code_rdd[j][1]\n",
    "details_rdd     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Karnataka': 'KA',\n",
       " 'Delhi': 'DL',\n",
       " 'Gujarat': 'GJ',\n",
       " 'Maharashtra': 'MH',\n",
       " 'Rajasthan': 'RJ',\n",
       " 'Punjab': 'PB',\n",
       " 'AndhraPradesh': 'AP',\n",
       " 'westBengal': 'WB'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = spark.sparkContext.textFile('State_codes.txt')\n",
    "stateKey = state.map(lambda word: (word.split(',')[0], word.split(',')[1]))\n",
    "\n",
    "Statecode_dict = {}\n",
    "for val in stateKey.collect():\n",
    "    Statecode_dict[val[0]] = val[1]\n",
    "    \n",
    "Statecode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subrahmanyan,04-12-1997,201046007,sub@gmail.com,Karnataka', 'Abhishek,15-10-1997,201051007,abhi@gmail.com,Delhi', 'Thejas,25-5-1997,201046013,thejas@gmail.com,AndhraPradesh', 'Rakhesh,22-03-1997,201051023,rakhi@gmail.com,Maharashtra', 'Shanthkumar,14-03-1999,201046049,shanthu@gmail.com,Gujarat', 'kiran,21-10-1997,2010156001,kiran@gmail.com,WestBengal', 'Anupam,05-09-1993,201056002,anupam@gmail.com,Rajasthan', 'Chandan,13-02-1997,201046003,chandan@gmail.com,Punjab']\n"
     ]
    }
   ],
   "source": [
    "mapData = spark.sparkContext.broadcast(Statecode_dict)\n",
    "\n",
    "city = spark.sparkContext.textFile('citizen.txt')\n",
    "print(city.collect())\n",
    "def compress(state,codes):\n",
    "    splitData = state.split(',')  \n",
    "    print(splitData)\n",
    "    splitData[4] = codes.value.get(splitData[4])\n",
    "    newData = ' '\n",
    "    newData = newData.join(splitData)\n",
    "    \n",
    "    return newData\n",
    "    \n",
    "Citizen = city.map(lambda data: compress(data,mapData))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}